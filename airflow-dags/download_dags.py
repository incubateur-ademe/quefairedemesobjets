import logging
import shutil
from datetime import datetime, timedelta
from pathlib import Path

import airflow.configuration as conf
import decouple
from airflow import DAG
from airflow.models import DagBag
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook


def download_dags_from_s3():
    dags_dirs = ["preprod", "production"]
    dags_folder = conf.get("core", "dags_folder")
    local_dags_folder = decouple.config("LOCAL_DAGS_FOLDER", cast=str, default="")
    if local_dags_folder:
        environment = "development"
        dags_dirs = [environment]
        logging.warning("Skipping download_dags_from_s3 in development environment")
        logging.warning(f"Copying dags from development to {dags_folder}")
        # copy all from HOME/development to dags_folder/development
        source = Path(str(local_dags_folder))
        destination = Path(dags_folder, environment)
        shutil.rmtree(destination, ignore_errors=True)
        shutil.copytree(source, destination)
    else:
        s3_hook = S3Hook(aws_conn_id="s3dags")
        bucket_name = "qfdmo-airflow-dags"
        keys = s3_hook.list_keys(bucket_name)
        for key in keys:
            logging.warning(f"Downloading {key} from S3 to {dags_folder}")
            file_path = Path(dags_folder, key)
            file_path.unlink(missing_ok=True)
            parent_folder = file_path.parent
            parent_folder.mkdir(parents=True, exist_ok=True)
            s3_hook.download_file(
                key,
                bucket_name=bucket_name,
                local_path=parent_folder,
                preserve_file_name=True,
                use_autogenerated_subdir=False,
            )
    for subdir in dags_dirs:
        logging.warning(f"Loading dags from {subdir}")
        dag_bag = DagBag(Path(dags_folder, subdir))
        if dag_bag:
            for dag_id, dag in dag_bag.dags.items():
                globals()[subdir + "_" + dag_id] = dag


default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2022, 1, 1),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    "download_dags_from_s3",
    default_args=default_args,
    description="DAG to download dags from S3",
    schedule_interval=timedelta(days=1),
    catchup=False,
) as dag:

    download_dags = PythonOperator(
        task_id="download_dags_from_s3", python_callable=download_dags_from_s3, dag=dag
    )

    download_dags
