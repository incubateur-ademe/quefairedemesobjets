# üìä Plateforme de donn√©es

```{toctree}
:hidden:

ingestion-de-source.md
flux-dbt.md
flux-enrich.md
```

Ce projet contient l'environnement d'execution d'Airflow

Les fichiers qui concerne la plateforme data :

- `pyproject.toml` d√©finition des d√©pendances dans la section [tool.poetry.group.airflow.dependencies]
- `./dags` r√©pertoire dans lequels sont stock√©s tous les dags execut√©s sur le cluster Airflow
- `airflow-scheduler.Dockerfile` et `airflow-webserver.Dockerfile`, fichier de configuration docker executer dans tous les environnements
- `docker-compose.yml` orchestre les dockers en envronnemnt de d√©veloppement
- `./dags_unit_tests` r√©pertoire qui contient les tests des dags

## Environnements

On distingue 3 environements:

- `development` : airflow tourne localement en utilisant l'orchestrateur `docker compose` (cf. [docker-compose.yml](../../../docker-compose.yml))
- `preprod` et `prod` : airflow tourne sur CleverCloud

## Mise √† jour du scheduler et du webserver sur CleverCloud

Airflow tourne sur CleverCloud sur un ensemble de serveur par environnement et utilise les services suivant:

- <ENVIRONMENT>-airflow-webserver (instance docker): interface d'airflow
- <ENVIRONMENT>-airflow-scheduler (instance docker): scheduler d'airflow, fait tourner les dags car on est configur√© en LocalExecutor
- <ENVIRONMENT>-airflow-s3 : S3 pour stocker les logs et les dags
- <ENVIRONMENT>-airflow-postgres : base de donn√©es n√©cessaire au fonctionnelment d'airflow

Les r√©pertoires s3 sont en cours de migration vers la plateforme Scaleway

## D√©ploiement et configuration

### CI/CD

la platefome Data est d√©ploy√©e en preprod √† chaque mise √† jour de la branche `main` sur Github (cf. [cd.yml](../../../.github/workflows/cd.yml))

Et en production √† chaque depot de tag de version (cf. [cd_prod.yml](../../../.github/workflows/cd.yml))

De la m√™me mani√®re que l'interface, cela permet de garder la coh√©rance entre l'application web et la plateforme data

### Configuration du cluster Airflow

#### Variable d'environnement du cluster Airflow

En plus des variables d'environnement n√©cessaire pour configurer Airflow, les variables d'environnemnt suivantes doivent-√™tre configur√©es sur le docker `scheduler` de chaque environnement dans CleverCloud.

Un exemple √† adapter selon l'environnement est disponible sur le fichier [.env.templates](../../../dags/.env.template)

#### Gestion des logs

Pour que les logs du scheduler soient stock√©s sur S3, les instances CleverCloud sont lanc√©s avec les variables d'environnement:

```
AIRFLOW__LOGGING__REMOTE_LOGGING=true
AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER=s3://qfdmo-airflow-logs
AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID=s3logs
AIRFLOW__LOGGING__ENCRYPT_S3_LOGS=false
```

`s3logs` est une connection configur√© dans l'interface d'Airflow

![Configuration d'une connexion √† Cellar (stockage s3 de clevercloud) dans Airflow](../../../img/airflow-s3-connection-configuration.png)

Attention √† ajouter le param√®tre endpoint_url pour le stockage Cellar de CleverCloud
