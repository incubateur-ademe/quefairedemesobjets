{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c30288f",
   "metadata": {},
   "source": [
    "# Dédupliquer à partir d'un excel\n",
    "\n",
    "A partir du fichier excel de cluster dédupliqué validé par Christian, nous créons un acteur chapeau et nous rattachons les acteurs du cluster à ce chapeau\n",
    "\n",
    "## Déclaration des librairies et des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab3dc870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "from shapely.geometry import Point\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "DATABASE_URL = 'postgresql+psycopg2://…'\n",
    "INTERFACE_HOST = \"http(s)://…\"\n",
    "# Chemin vers le fichier Excel\n",
    "# FILE_PATH = '../cluster dechetteries - check_similarity_downstream_without_filtering_by_threshold.xlsx'\n",
    "# FILE_PATH = '../duplicated_commerce_actors_threhsold_0_8_11_09_2024.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876c097",
   "metadata": {},
   "source": [
    "## Connection à la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6d5405-0e71-4f1b-9acb-c3ef733c900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the engine\n",
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7f32b",
   "metadata": {},
   "source": [
    "## Récupération des id des sources ALIAPUR et COREPILE\n",
    "\n",
    "Car on sait par expérience que ces 2 Eco-organismes ont des données plus propre, on préfèrera donc utiliser leur données plutôt que celle des autres Eco-organisme en cas de choix à faire, champ par champ pour créer l'acteur chapeau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fa217ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des id des sources aliapur et corepile\n",
    "query_acteur = \"SELECT id, code FROM qfdmo_source WHERE code IN ('ALIAPUR','COREPILE');\"\n",
    "sources = pd.read_sql_query(query_acteur, engine)\n",
    "trusted_sources = ['ALIAPUR','COREPILE']\n",
    "trusted_source_ids = sources[sources['code'].isin(trusted_sources)]['id'].tolist()\n",
    "\n",
    "# Ajouter l'id 0 au début de la liste trusted_source_ids pour donner la priorité\n",
    "# aux acteurs chapeau déjà existant\n",
    "trusted_source_ids.insert(0, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba3b7e",
   "metadata": {},
   "source": [
    "## Gestion du fichier de déduplication\n",
    "\n",
    "On récupère le fichier à dédupliquer localement et on le transforme en dataframe pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e514ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire le fichier Excel et le convertir en DataFrame\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# On ne récupère ques les clusters d'identifiant_unique, le reste est retrouvé via\n",
    "# la base de données\n",
    "df_clusters = df[['cluster_id', 'identifiant_unique']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a8bf5",
   "metadata": {},
   "source": [
    "## Récupération des acteurs et revisionacteurs de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ec81997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtré par identifiant_unique\n",
    "identifiant_unique = df_clusters['identifiant_unique'].tolist()\n",
    "identifiant_unique_list = [f\"'{i}'\" for i in identifiant_unique]\n",
    "\n",
    "query_acteur = f\"SELECT * FROM qfdmo_acteur WHERE identifiant_unique IN ({','.join(identifiant_unique_list)});\"\n",
    "df_acteur_from_sql = pd.read_sql_query(query_acteur, engine)\n",
    "\n",
    "query_revisionacteur = f\"SELECT * FROM qfdmo_revisionacteur WHERE identifiant_unique IN ({','.join(identifiant_unique_list)});\"\n",
    "df_revisionacteur_from_sql = pd.read_sql_query(query_revisionacteur, engine)\n",
    "\n",
    "# merge acteur et revisionacteur\n",
    "df_acteur_from_sql['parent_id'] = None\n",
    "df_acteur_from_sql.set_index(\"identifiant_unique\", inplace=True)\n",
    "df_revisionacteur_from_sql.set_index(\"identifiant_unique\", inplace=True)\n",
    "\n",
    "# mise à jour avec les données de revisionacteur si elles existent\n",
    "df_acteur_from_sql.update(df_revisionacteur_from_sql)\n",
    "df_acteur_from_sql.reset_index(inplace=True)\n",
    "df_acteur_from_sql['is_parent'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d19467b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des parents\n",
    "df_parentacteur_from_sql = pd.DataFrame(\n",
    "    columns=df_acteur_from_sql.columns\n",
    ")\n",
    "df_enfantacteur_from_sql = pd.DataFrame(\n",
    "    columns=df_acteur_from_sql.columns\n",
    ")\n",
    "\n",
    "parent_ids = df_revisionacteur_from_sql['parent_id'].dropna().unique().tolist()\n",
    "parent_id_list = [f\"'{i}'\" for i in parent_ids]\n",
    "if parent_ids:\n",
    "    query_parentacteur = f\"SELECT * FROM qfdmo_revisionacteur WHERE identifiant_unique IN ({','.join(parent_id_list)});\"\n",
    "    df_parentacteur_from_sql = pd.read_sql_query(query_parentacteur, engine)\n",
    "    df_parentacteur_from_sql['source_id'] = 0\n",
    "    df_parentacteur_from_sql['is_parent'] = True\n",
    "\n",
    "    # récupération des enfants des parents pour les inclure dans les clusters\n",
    "    query_enfantacteur = f\"SELECT * FROM qfdmo_revisionacteur WHERE parent_id IN ({','.join(parent_id_list)});\"\n",
    "    df_enfantacteur_from_sql = pd.read_sql_query(query_enfantacteur, engine)\n",
    "    df_enfantacteur_from_sql['is_parent'] = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd66309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout du cluster_id dans les acteurs\n",
    "df_acteur_from_sql = df_acteur_from_sql.merge(df_clusters, on='identifiant_unique', how='left')\n",
    "\n",
    "# Ajout du cluster_id dans les parents en inspectant si le cluster des enfant dans df_acteur_from_sql\n",
    "df_parentacteur_from_sql[\"cluster_id\"] = df_parentacteur_from_sql['identifiant_unique'].apply(\n",
    "    lambda x: df_acteur_from_sql[df_acteur_from_sql['parent_id'] == x]['cluster_id'].values[0])\n",
    "\n",
    "# ajout du cluster_id dans les enfants en inspectant le cluster_id de leur parent\n",
    "df_enfantacteur_from_sql[\"cluster_id\"] = df_enfantacteur_from_sql['parent_id'].apply(\n",
    "    lambda identifiant_unique: df_parentacteur_from_sql[df_parentacteur_from_sql['identifiant_unique'] == identifiant_unique]['cluster_id'].values[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae961d5",
   "metadata": {},
   "source": [
    "## Compilation des acteurs chapeau\n",
    "\n",
    "- Regroupement des acteurs par cluster\n",
    "- Choix de la valeur à appliquer pour chacun des champs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a3d3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acteur_merged = pd.concat([df_acteur_from_sql, df_parentacteur_from_sql, df_enfantacteur_from_sql])\n",
    "df_acteur_merged.drop_duplicates(subset=['identifiant_unique'], inplace=True)\n",
    "grouped_df = df_acteur_merged.groupby('cluster_id').apply(lambda x: x.to_dict(\"records\") if not x.empty else [])\n",
    "\n",
    "# Ordonner chaque groupe par ordre de confiance selon la liste trusted_sources : \n",
    "# D'abord la source ALIAPUR, puis COREPILE, puis les autres sources\n",
    "grouped_df = grouped_df.apply( \n",
    "    lambda x: (\n",
    "        sorted(x, key= lambda y : trusted_source_ids.index(y['source_id']) if y['source_id'] in trusted_source_ids else len(trusted_source_ids))\n",
    "    )\n",
    ")\n",
    "\n",
    "acteurchapeau_par_cluster = {}\n",
    "for cluster_id, group in grouped_df.items():\n",
    "    # Pour chacune des colonnes columns\n",
    "    # On prend dans l'ordre la valeur de la première ligne (group) si elle est non nulle\n",
    "    # Puis la valeur de la suivante si elle est non nulle\n",
    "    # etc\n",
    "    combined_row = {}\n",
    "    for column in df_acteur_merged.columns:\n",
    "        for record in group:\n",
    "            if pd.notnull(record[column]) and record[column] != '':\n",
    "                combined_row[column] = record[column]\n",
    "                break\n",
    "    \n",
    "    identifiant_uniques = sorted([\n",
    "        record['identifiant_unique'] for record in group if record['is_parent'] is False\n",
    "    ])\n",
    "\n",
    "    combined_row['identifiant_unique'] = str(\n",
    "        uuid.uuid5(uuid.NAMESPACE_DNS, identifiant_uniques[0])\n",
    "    )\n",
    "    acteurchapeau_par_cluster[cluster_id] = combined_row\n",
    "\n",
    "#acteurs_chapeau_par_cluster = pd.DataFrame(acteurs_chapeau_par_cluster)\n",
    "acteurs_chapeau = acteurchapeau_par_cluster.values()\n",
    "df_acteurchapeau = pd.DataFrame(acteurs_chapeau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34336656",
   "metadata": {},
   "source": [
    "## Création des acteurs chapeau en base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "630e3f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2649"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppression des colonnes qui ne sont pas en base de données\n",
    "cleaned_acteurs_chapeau_df = df_acteurchapeau.drop(\n",
    "    columns=['is_parent', 'cluster_id', 'source_id', 'identifiant_externe']\n",
    ")\n",
    "# Write to the database\n",
    "cleaned_acteurs_chapeau_df.to_sql(\n",
    "    'qfdmo_revisionacteur',\n",
    "    engine,\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=\"multi\",\n",
    "    chunksize=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3f25c",
   "metadata": {},
   "source": [
    "## Créaction et mise à jour des RevisionActeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a74f63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retries = 3\n",
    "from tenacity import retry, wait_fixed, stop_after_attempt, retry_if_exception_type\n",
    "@retry(\n",
    "    wait=wait_fixed(5),\n",
    "    stop=stop_after_attempt(3),\n",
    "    retry=retry_if_exception_type(Exception)\n",
    ")\n",
    "def get_or_create_revisionacteur(identifiant_unique : str) -> None:\n",
    "    response = requests.get(\n",
    "        f\"{INTERFACE_HOST}/qfdmo/getorcreate_revisionacteur/{identifiant_unique}\"\n",
    "    )\n",
    "    if response.status_code >= 400:\n",
    "        raise Exception(f\"Error while getting or creating revisionacteur for {identifiant_unique}\")\n",
    "\n",
    "for cluster_id, group in grouped_df.items():\n",
    "    for record in group:\n",
    "        if record['is_parent']:\n",
    "            continue\n",
    "        # Creation du revision acteur pour chaque acteur à dédupliquer\n",
    "        get_or_create_revisionacteur(record['identifiant_unique'])\n",
    "\n",
    "        # Mise à jour du parent_id avec l'identifiant_unique de l'acteur chapeau\n",
    "        parent_id = acteurchapeau_par_cluster[cluster_id]['identifiant_unique']\n",
    "        query_acteur = f\"\"\"\n",
    "            UPDATE qfdmo_revisionacteur\n",
    "            SET parent_id = '{parent_id}'\n",
    "            WHERE identifiant_unique = '{record['identifiant_unique']}';\n",
    "        \"\"\"\n",
    "        engine.execute(query_acteur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c949fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old parent\n",
    "if parent_ids:\n",
    "    query_delete_old_parentacteur = f\"\"\"DELETE FROM qfdmo_revisionacteur WHERE identifiant_unique IN ({','.join(parent_id_list)});\"\"\"\n",
    "    engine.execute(query_delete_old_parentacteur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ba6d0",
   "metadata": {},
   "source": [
    "# Outils pour tester\n",
    "\n",
    "Supprimer tous les acteurs chapeau qui viennent d'être créés.\n",
    "\n",
    "⚠️ Ne pas utiliser hors de l'environnement de développement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ba9bb5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x120ae0b60>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tooling temporaire\n",
    "\n",
    "\n",
    "# supprimer les acteurs chapeau identifié pas cleaned_acteurs_chapeau_df['identifiant_unique']\n",
    "identifiant_uniques = cleaned_acteurs_chapeau_df['identifiant_unique'].tolist()\n",
    "query_acteur = f\"UPDATE qfdmo_revisionacteur SET parent_id = NULL WHERE parent_id IN ({', '.join(f\"'{id}'\" for id in identifiant_uniques)});\"\n",
    "engine.execute(query_acteur)\n",
    "query_acteur = f\"DELETE FROM qfdmo_revisionacteur WHERE identifiant_unique IN ({', '.join(f\"'{id}'\" for id in identifiant_uniques)});\"\n",
    "engine.execute(query_acteur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097d1c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
