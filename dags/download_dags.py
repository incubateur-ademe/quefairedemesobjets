# FIXME : Ce DAG devrait être remplacer par un cron
import logging
import shutil
from datetime import datetime, timedelta
from pathlib import Path

import airflow.configuration as conf
import decouple
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

BUCKET_NAME = "qfdmo-airflow-dags"


def download_dags_from_s3():
    dags_folder = conf.get("core", "dags_folder")
    if not isinstance(dags_folder, str):
        raise ValueError("dags_folder is not set")
    environment = decouple.config("ENVIRONMENT", default=None)
    if not environment:
        raise ValueError("ENVIRONMENT is not set")
    if environment in ["preprod", "production"]:
        s3_hook = S3Hook(aws_conn_id="s3dags")

        logging.warning(f"Remove old dags from {dags_folder}")
        folder_path = Path(dags_folder)
        files_to_keep = [".env", Path(__file__).name]
        for item in folder_path.iterdir():
            # Vérifier si l'élément est un fichier ou un dossier
            if item.is_file():
                # Supprimer le fichier s'il ne correspond pas au nom spécifié
                if item.name not in files_to_keep:
                    item.unlink()
            elif item.is_dir():
                # Supprimer le dossier
                shutil.rmtree(item)
        keys = s3_hook.list_keys(BUCKET_NAME, prefix=f"{environment}/")

        logging.warning(f"Keys in {BUCKET_NAME}: {keys}")
        logging.warning(
            f"Downloading {BUCKET_NAME}/{environment} from S3 to {dags_folder}"
        )
        for key in keys:
            local_key = key.replace(f"{environment}/", "")
            file_path = Path(dags_folder, local_key)
            logging.warning(f"Downloading {key} from S3 to {file_path}")
            file_path.unlink(missing_ok=True)
            logging.warning(f"Creating {file_path}")
            parent_folder = file_path.parent
            parent_folder.mkdir(parents=True, exist_ok=True)
            s3_hook.download_file(
                key,
                bucket_name=BUCKET_NAME,
                local_path=parent_folder,
                preserve_file_name=True,
                use_autogenerated_subdir=False,
            )
    else:
        logging.warning(f"Skipping download_dags_from_s3 in environmen `{environment}`")


default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2022, 1, 1),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    "download_dags_from_s3",
    default_args=default_args,
    description="DAG to download dags from S3",
    schedule_interval=timedelta(days=1),
    catchup=False,
) as dag:

    download_dags = PythonOperator(
        task_id="download_dags_from_s3", python_callable=download_dags_from_s3, dag=dag
    )

    download_dags
