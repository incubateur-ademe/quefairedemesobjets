ENVIRONMENT=development
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags

CELLAR_ADDON_HOST=<CELLAR_ADDON_HOST>
CELLAR_ADDON_KEY_ID=<CELLAR_ADDON_KEY_ID>
CELLAR_ADDON_KEY_SECRET=<CELLAR_ADDON_KEY_SECRET>
CELLAR_ADDON_BUCKET=<CELLAR_ADDON_BUCKET>
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow # pragma: allowlist secret
AIRFLOW__CORE__DAGS_FOLDER='/opt/airflow/dags'
AIRFLOW__CORE__FERNET_KEY=''
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION='true'
AIRFLOW__CORE__LOAD_EXAMPLES='false'
AIRFLOW__CORE__ENABLE_XCOM_PICKLING='true'
AIRFLOW__WEBSERVER__EXPOSE_CONFIG='true'
AIRFLOW__WEBSERVER__WORKERS='2'
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__API__AUTH_BACKENDS='airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
# Uncomment the next 4 lines to store logs in S3
# AIRFLOW__LOGGING__REMOTE_LOGGING='true'
# AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER="s3://qfdmo-airflow-logs"
# AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID='s3logs'
# AIRFLOW__LOGGING__ENCRYPT_S3_LOGS="false"
AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK='true'
# WARNING=Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
# for other purpose (development, test and especially production usage) build/extend Airflow image.
_PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS:-}
AIRFLOW_CONN_QFDMO-DJANGO-DB='postgres://qfdmo:qfdmo@lvao-db:5432/qfdmo' # pragma: allowlist secret
