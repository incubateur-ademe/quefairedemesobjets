ENVIRONMENT=development
# Nécessaire car variable dans /core/settings.py
# qu'on a souhaité laissé nulle par défaut
# pour des raisons de sécurité
# Voir PR: https://github.com/incubateur-ademe/quefairedemesobjets/pull/1189
SECRET_KEY='my-secret-key' # pragma: allowlist secret
CELLAR_ADDON_HOST=<CELLAR_ADDON_HOST>
CELLAR_ADDON_KEY_ID=<CELLAR_ADDON_KEY_ID>
CELLAR_ADDON_KEY_SECRET=<CELLAR_ADDON_KEY_SECRET>
CELLAR_ADDON_BUCKET=<CELLAR_ADDON_BUCKET>

# AIRFLOW
_AIRFLOW_DB_MIGRATE=true

## CORE
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
# following https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/fernet.html
AIRFLOW__CORE__FERNET_KEY=''
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION='true'
AIRFLOW__CORE__LOAD_EXAMPLES='false'
AIRFLOW__CORE__ENABLE_XCOM_PICKLING='true'
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__AUTH_MANAGER='airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
AIRFLOW__CORE__EXECUTION_API_SERVER_URL=http://airflow-webserver:8080/execution/

## API
AIRFLOW__API__AUTH_BACKENDS='airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
AIRFLOW__API_AUTH__JWT_SECRET='<jwt-token-blah-blah-blah>'

## WEBSERVER
AIRFLOW__WEBSERVER__EXPOSE_CONFIG='true'
AIRFLOW__WEBSERVER__WORKERS='2'

## LOGGER
# Uncomment the next 4 lines to store logs in S3
AIRFLOW__LOGGING__REMOTE_LOGGING='true'
AIRFLOW__LOGGING__WRITE_STDOUT='true'
AIRFLOW__LOGGING__JSON_FORMAT='true'

## ELASTICSEARCH
AIRFLOW__ELASTICSEARCH__HOST=elasticsearch:9200
AIRFLOW__ELASTICSEARCH__WRITE_STDOUT='true'
AIRFLOW__ELASTICSEARCH__WRITE_TO_ES='true'
AIRFLOW__ELASTICSEARCH__JSON_FORMAT='true'
AIRFLOW__ELASTICSEARCH__LOG_ID_TEMPLATE="{dag_id}-{task_id}-{execution_date}-{try_number}"
AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_CONFIGS='{"max_retries": 3, "timeout": 30}'

## SCHEDULER
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=3
AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK='true'
AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT='false'

## DATABASE
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow # pragma: allowlist secret
AIRFLOW__DATABASE__SQL_ALCHEMY_CONNECT_ARGS=airflow_local_settings.keepalive_kwargs

### CONNECTOR
AIRFLOW_CONN_WEBAPP_DB='postgres://webapp:webapp@lvao-webapp-db:5432/webapp' # pragma: allowlist secret

# DOCKER / AIRFLOW

# WARNING=Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
# for other purpose (development, test and especially production usage) build/extend Airflow image.
_PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS:-}

AIRFLOW_CONN_WEBAPP_DB='postgres://webapp:webapp@lvao-webapp-db:5432/webapp' # pragma: allowlist secret
DATABASE_URL=postgres://webapp:webapp@lvao-webapp-db:5432/webapp # pragma: allowlist secret
DB_WAREHOUSE=postgres://warehouse:warehouse@lvao-warehouse-db:5432/warehouse # pragma: allowlist secret

# DBT env vars
POSTGRES_HOST=lvao-webapp-db
POSTGRES_PORT=5432
POSTGRES_USER=webapp
POSTGRES_PASSWORD=webapp # pragma: allowlist secret
POSTGRES_DB=warehouse
POSTGRES_SCHEMA=public

# SCALEWAY
SCW_ACCESS_KEY=
SCW_SECRET_KEY=
SCW_DEFAULT_ORGANIZATION_ID=
SCW_DEFAULT_PROJECT_ID=
