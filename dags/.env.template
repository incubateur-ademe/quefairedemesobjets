ENVIRONMENT=development
# Nécessaire car variable dans /core/settings.py
# qu'on a souhaité laissé nulle par défaut
# pour des raisons de sécurité
# Voir PR: https://github.com/incubateur-ademe/quefairedemesobjets/pull/1189
SECRET_KEY='my-secret-key' # pragma: allowlist secret
CELLAR_ADDON_HOST=<CELLAR_ADDON_HOST>
CELLAR_ADDON_KEY_ID=<CELLAR_ADDON_KEY_ID>
CELLAR_ADDON_KEY_SECRET=<CELLAR_ADDON_KEY_SECRET>
CELLAR_ADDON_BUCKET=<CELLAR_ADDON_BUCKET>

# AIRFLOW

## CORE
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
AIRFLOW__CORE__FERNET_KEY=''
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION='true'
AIRFLOW__CORE__LOAD_EXAMPLES='false'
AIRFLOW__CORE__ENABLE_XCOM_PICKLING='true'
AIRFLOW__CORE__EXECUTOR=LocalExecutor

## API
AIRFLOW__API__AUTH_BACKENDS='airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'

## WEBSERVER
AIRFLOW__WEBSERVER__EXPOSE_CONFIG='true'
AIRFLOW__WEBSERVER__WORKERS='2'

## LOGGER
# Uncomment the next 4 lines to store logs in S3
# AIRFLOW__LOGGING__REMOTE_LOGGING='true'
# AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER="s3://qfdmo-airflow-logs"
# AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID='s3logs'
# AIRFLOW__LOGGING__ENCRYPT_S3_LOGS="false"

## SCHEDULER
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=3
AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK='true'

## DATABASE
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow # pragma: allowlist secret

### CONNECTOR
AIRFLOW_CONN_QFDMO_DJANGO_DB='postgres://qfdmo:qfdmo@lvao-db:5432/qfdmo' # pragma: allowlist secret

# DOCKER / AIRFLOW

# WARNING=Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
# for other purpose (development, test and especially production usage) build/extend Airflow image.
_PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS:-}

DATABASE_URL=postgis://qfdmo:qfdmo@lvao-db:5432/qfdmo # pragma: allowlist secret
