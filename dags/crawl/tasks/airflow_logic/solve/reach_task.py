import logging

import pandas as pd
from airflow import DAG
from airflow.exceptions import AirflowSkipException
from airflow.operators.python import PythonOperator
from crawl.tasks.airflow_logic import task_ids as TASK_IDS
from crawl.tasks.business_logic.solve.reach import crawl_urls_solve_reach
from utils import logging_utils as log

logger = logging.getLogger(__name__)


def task_info_get():
    return f"""


    ============================================================
    Description de la tÃ¢che "{TASK_IDS.SOLVE_REACH}"
    ============================================================

    ðŸ’¡ quoi: on essaye de parcourir les URLs

    ðŸŽ¯ pourquoi: proposer des suggestions Ã  l'Ã©tape d'aprÃ¨s sur
    les URLs qu'on a rÃ©ussit Ã  parcourir

    ðŸ—ï¸ comment: avec un crawler python tout simple
    """


def crawl_urls_solve_reach_wrapper(**kwargs) -> None:
    logger.info(task_info_get())

    df = kwargs["ti"].xcom_pull(key="df", task_ids=TASK_IDS.SOLVE_SYNTAX)
    if not isinstance(df, pd.DataFrame) or df.empty:
        msg = f"df de {TASK_IDS.SOLVE_SYNTAX} vide: on devrait pas Ãªtre ici"
        raise ValueError(msg)

    log.preview_df_as_markdown("ðŸŸ¢ URLs qu'on va essayer de parcourir", df)
    df_ok_same, df_ok_diff, df_fail = crawl_urls_solve_reach(df)

    logging.info(log.banner_string("ðŸ RÃ©sultat final de cette tÃ¢che"))
    log.preview_df_as_markdown("ðŸŸ¢ URLs en succÃ¨s ET inchangÃ©es", df_ok_same)
    log.preview_df_as_markdown("ðŸŸ¡ URLs en succÃ¨s ET diffÃ©rentes", df_ok_diff)
    log.preview_df_as_markdown("ðŸ”´ URLs en Ã©chec", df_fail)

    if df_ok_diff.empty and df_fail.empty:
        msg = "0ï¸âƒ£ Pas d'URLs en succÃ¨s diff/Ã©chec = pas de suggestions"
        raise AirflowSkipException(msg)

    kwargs["ti"].xcom_push(key="df_ok_same", value=df_ok_same)
    kwargs["ti"].xcom_push(key="df_ok_diff", value=df_ok_diff)
    kwargs["ti"].xcom_push(key="df_fail", value=df_fail)


def crawl_urls_solve_reach_task(dag: DAG) -> PythonOperator:
    return PythonOperator(
        task_id=TASK_IDS.SOLVE_REACH,
        python_callable=crawl_urls_solve_reach_wrapper,
        dag=dag,
    )
