import logging

import crawl.tasks.airflow_logic.task_ids as TASK_IDS
from airflow import DAG
from airflow.operators.python import PythonOperator
from crawl.tasks.business_logic.candidates.groupby_url import (
    crawl_urls_candidates_groupby_url,
)
from utils import logging_utils as log

logger = logging.getLogger(__name__)


def task_info_get():
    return f"""


    ============================================================
    Description de la tÃ¢che "{TASK_IDS.GROUPBY_URL}"
    ============================================================

    ðŸ’¡ quoi: regroupement par URL

    ðŸŽ¯ pourquoi: pas faire un doublon de travail, on traite
    par URL et non pas par entrÃ©e (ex: acteurs)

    ðŸ—ï¸ comment: on groupe par URL et aggrÃ¨ge toutes les entrÃ©es
    associÃ©es (ex: acteurs) pour gÃ©nÃ©rer des suggestions pour
    toute ces entrÃ©es plus tard
    """


def crawl_urls_candidates_groupby_url_wrapper(**kwargs) -> None:
    logger.info(task_info_get())

    df = kwargs["ti"].xcom_pull(key="df", task_ids=TASK_IDS.READ)
    df = crawl_urls_candidates_groupby_url(
        df, col_groupby="url_original", col_grouped_data="acteurs"
    )

    logging.info(log.banner_string("ðŸ RÃ©sultat final de cette tÃ¢che"))
    log.preview_df_as_markdown("URLs regroupÃ©es", df)

    if df.empty:
        raise ValueError("Si est on arrivÃ© ici, on devrait avoir des URLs")

    kwargs["ti"].xcom_push(key="df", value=df)


def crawl_urls_candidates_groupby_url_task(dag: DAG) -> PythonOperator:
    return PythonOperator(
        task_id=TASK_IDS.GROUPBY_URL,
        python_callable=crawl_urls_candidates_groupby_url_wrapper,
        dag=dag,
    )
