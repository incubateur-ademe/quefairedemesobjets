import logging

import crawl.tasks.airflow_logic.task_ids as TASK_IDS
from airflow import DAG
from airflow.exceptions import AirflowSkipException
from airflow.operators.python import PythonOperator
from crawl.tasks.business_logic.candidates.read_from_db import (
    crawl_urls_candidates_read_from_db,
)
from utils import logging_utils as log

logger = logging.getLogger(__name__)


def task_info_get(url_type: str):
    return f"""


    ============================================================
    Description de la tÃ¢che "{TASK_IDS.READ}"
    ============================================================

    ðŸ’¡ quoi: sÃ©lection d'URLs Ã  parcourir dans {url_type}

    ðŸŽ¯ pourquoi: pour pouvoir les parcourir

    ðŸ—ï¸ comment: on va chercher en DB Ã  partir du type d'URL
    """


def crawl_urls_candidates_read_from_db_wrapper(**kwargs) -> None:
    params = kwargs["params"]
    logger.info(task_info_get(params["urls_type"]))

    df = crawl_urls_candidates_read_from_db(params["urls_type"], params["urls_limit"])

    logging.info(log.banner_string("ðŸ RÃ©sultat final de cette tÃ¢che"))
    log.preview_df_as_markdown("URLs Ã  parcourir", df)

    if df.empty:
        raise AirflowSkipException("Pas d'URLs Ã  parcourir = on s'arrÃªte lÃ ")

    kwargs["ti"].xcom_push(key="df", value=df)


def crawl_urls_candidates_read_from_db_task(dag: DAG) -> PythonOperator:
    return PythonOperator(
        task_id=TASK_IDS.READ,
        python_callable=crawl_urls_candidates_read_from_db_wrapper,
        dag=dag,
    )
